{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":11017890,"sourceType":"datasetVersion","datasetId":6860298},{"sourceId":11036714,"sourceType":"datasetVersion","datasetId":6874312},{"sourceId":11045044,"sourceType":"datasetVersion","datasetId":6880199},{"sourceId":11049091,"sourceType":"datasetVersion","datasetId":6883195},{"sourceId":11055588,"sourceType":"datasetVersion","datasetId":6887807},{"sourceId":11113694,"sourceType":"datasetVersion","datasetId":6929273},{"sourceId":11192316,"sourceType":"datasetVersion","datasetId":6987097},{"sourceId":11275614,"sourceType":"datasetVersion","datasetId":7049098},{"sourceId":11278122,"sourceType":"datasetVersion","datasetId":7050929},{"sourceId":11278172,"sourceType":"datasetVersion","datasetId":7050970}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"import fastai\nfrom fastai.vision.all import *\nfrom tqdm import tqdm\nfrom glob import glob","metadata":{"_uuid":"e97fa043-02da-4543-b1ac-513ffc44dec2","_cell_guid":"55935d31-2694-45da-a34a-94261649439c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:56.412007Z","iopub.execute_input":"2025-11-04T16:33:56.412566Z","iopub.status.idle":"2025-11-04T16:33:56.418373Z","shell.execute_reply.started":"2025-11-04T16:33:56.412540Z","shell.execute_reply":"2025-11-04T16:33:56.417545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 85\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)","metadata":{"_uuid":"08422815-d5d5-4f12-93bf-58e031e70dda","_cell_guid":"738a2e57-62b1-4cd1-970c-e9505a16a5b5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:56.419527Z","iopub.execute_input":"2025-11-04T16:33:56.419773Z","iopub.status.idle":"2025-11-04T16:33:56.436156Z","shell.execute_reply.started":"2025-11-04T16:33:56.419753Z","shell.execute_reply":"2025-11-04T16:33:56.435522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_SIZE = [224, 224]\nBATCH_SIZE = 128\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:56.437216Z","iopub.execute_input":"2025-11-04T16:33:56.437541Z","iopub.status.idle":"2025-11-04T16:33:56.452587Z","shell.execute_reply.started":"2025-11-04T16:33:56.437520Z","shell.execute_reply":"2025-11-04T16:33:56.451770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_train_val = pd.read_csv('/kaggle/input/data/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\nlabels_test = pd.read_csv('/kaggle/input/data/test_list.txt')\nlabels_test.columns = ['Image_Index']\ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']\n# NIH Dataset Labels CSV File \nlabels_df = pd.read_csv('/kaggle/input/data/Data_Entry_2017.csv')\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']\n# One hot encoding\nfor diseases in tqdm(disease_labels): \n    labels_df[diseases] = labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0)\n\n# labels_df.to_csv('/kaggle/working/newData.csv')\nlabels_df=labels_df[labels_df.Finding_Labels != 'No Finding']\n#labels_df.head(3)","metadata":{"_uuid":"a146646c-ca7b-4441-869a-7407da42f4b0","_cell_guid":"4e4d9570-14a8-46bd-a77b-4c59057d578e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:56.453715Z","iopub.execute_input":"2025-11-04T16:33:56.453995Z","iopub.status.idle":"2025-11-04T16:33:57.198079Z","shell.execute_reply.started":"2025-11-04T16:33:56.453943Z","shell.execute_reply":"2025-11-04T16:33:57.197236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlabels_df['Finding_Labels'] = labels_df['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\nnum_glob = glob('/kaggle/input/data/*/images/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\nlabels_df['Paths'] = labels_df['Image_Index'].map(img_path.get)\nlabels_df.head()","metadata":{"_uuid":"5a632ffd-8096-481a-bc78-4192c1663625","_cell_guid":"e00c2fca-fb51-4231-a493-2b62f43b83eb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.199415Z","iopub.execute_input":"2025-11-04T16:33:57.199658Z","iopub.status.idle":"2025-11-04T16:33:57.529270Z","shell.execute_reply.started":"2025-11-04T16:33:57.199640Z","shell.execute_reply":"2025-11-04T16:33:57.528557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_patients = np.unique(labels_df['Patient_ID'])\nlen(unique_patients)","metadata":{"_uuid":"ba306f3c-3183-4087-8143-e385f83a22df","_cell_guid":"1052fa10-fd86-4042-ae7b-ec09fbba0d86","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.530041Z","iopub.execute_input":"2025-11-04T16:33:57.530304Z","iopub.status.idle":"2025-11-04T16:33:57.537260Z","shell.execute_reply.started":"2025-11-04T16:33:57.530283Z","shell.execute_reply":"2025-11-04T16:33:57.536450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# train-70\n# val-10\n# test-20\ntrain_val_df_patients, test_df_patients = train_test_split(unique_patients, \n                                   test_size = 0.2,\n                                   random_state = SEED,\n                                    shuffle= True\n                                   )\nlen(train_val_df_patients)","metadata":{"_uuid":"31d9e8b0-d79f-4b79-8f89-2bfaa6e7ab99","_cell_guid":"8ff1d9c5-1b3e-4747-8716-f0c811db89d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.539417Z","iopub.execute_input":"2025-11-04T16:33:57.539655Z","iopub.status.idle":"2025-11-04T16:33:57.552533Z","shell.execute_reply.started":"2025-11-04T16:33:57.539639Z","shell.execute_reply":"2025-11-04T16:33:57.551725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_val_df = labels_df[labels_df['Patient_ID'].isin(train_val_df_patients)]","metadata":{"_uuid":"5060ce8a-6576-434f-93f5-927f625d461a","_cell_guid":"dc7d2e26-3cd9-4631-9fc5-62e3b856893a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.553302Z","iopub.execute_input":"2025-11-04T16:33:57.553811Z","iopub.status.idle":"2025-11-04T16:33:57.580111Z","shell.execute_reply.started":"2025-11-04T16:33:57.553787Z","shell.execute_reply":"2025-11-04T16:33:57.579446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_val_df.head()","metadata":{"_uuid":"ce4122ec-3f90-4da8-ae78-6fb3ed4921c7","_cell_guid":"47af73dc-5657-479e-9a12-cf79a4f16807","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.580862Z","iopub.execute_input":"2025-11-04T16:33:57.581107Z","iopub.status.idle":"2025-11-04T16:33:57.602703Z","shell.execute_reply.started":"2025-11-04T16:33:57.581085Z","shell.execute_reply":"2025-11-04T16:33:57.602002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_df.shape\nprint('train_val size', train_val_df.shape[0])\nprint('test size', labels_df.shape[0] - train_val_df.shape[0])","metadata":{"_uuid":"d01adf34-18c5-49af-b3b5-338879c7cb31","_cell_guid":"1f8001a9-76cc-47ad-82af-9402d77c6173","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.603372Z","iopub.execute_input":"2025-11-04T16:33:57.603710Z","iopub.status.idle":"2025-11-04T16:33:57.609009Z","shell.execute_reply.started":"2025-11-04T16:33:57.603687Z","shell.execute_reply":"2025-11-04T16:33:57.608165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data builder","metadata":{}},{"cell_type":"code","source":"item_transforms = [\n    Resize((224, 224)),\n]\n\nbatch_transforms = [\n    Flip(),\n    Rotate(),\n    Normalize.from_stats(*imagenet_stats),\n]\n\n\ndef get_x(row):\n    return row['Paths']\n\ndef get_y(row):\n    labels = row[disease_labels].tolist()\n    return labels\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock(encoded=True,vocab=disease_labels)),\n                   splitter=RandomSplitter(valid_pct=0.125, seed=SEED),\n                   get_x=get_x,\n                   get_y=get_y,\n                   item_tfms=item_transforms,\n                   batch_tfms=batch_transforms\n                  )\ndls = dblock.dataloaders(train_val_df, bs=64)\n# print(dblock.datasets(train_val_merge).train)","metadata":{"_uuid":"db273220-b8dd-4b32-b257-3e4f2a5f8cfa","_cell_guid":"a56c961f-a443-4ae9-8f9a-a52be3e90b99","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.609867Z","iopub.execute_input":"2025-11-04T16:33:57.610155Z","iopub.status.idle":"2025-11-04T16:33:57.873340Z","shell.execute_reply.started":"2025-11-04T16:33:57.610130Z","shell.execute_reply":"2025-11-04T16:33:57.872565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.nn import functional as F\nfrom copy import deepcopy\n\n\n# Memory Bank: Store rare/important features\nclass MemoryBank(nn.Module):\n    def __init__(self, feature_dim, bank_size=1000, update_strategy='rarity', \n                 rarity_threshold=0.2, diversity_weight=0.5, momentum=0.9):\n        super(MemoryBank, self).__init__()\n        self.feature_dim = feature_dim\n        self.bank_size = bank_size\n        self.update_strategy = update_strategy\n        self.rarity_threshold = rarity_threshold\n        self.diversity_weight = diversity_weight\n        self.momentum = momentum\n        \n        self.register_buffer('memory', torch.zeros(bank_size, feature_dim))\n        self.register_buffer('index', torch.tensor(0))\n        self.register_buffer('memory_count', torch.zeros(bank_size))\n        self.register_buffer('running_mean', torch.zeros(feature_dim))\n        self.register_buffer('running_var', torch.ones(feature_dim))\n        self.register_buffer('num_updates', torch.tensor(0))\n        \n        # For statistical strategy - track running mean of norms\n        self.register_buffer('running_mean_norm', torch.tensor(0.0))\n\n    def compute_importance_scores(self, features):\n        \"\"\"Compute importance scores based on selected strategy\"\"\"\n        batch_size = features.size(0)\n        \n        if self.update_strategy == 'rarity':\n            # Original: Based on L2 norm deviation from mean\n            mean_norm = torch.mean(torch.norm(features, dim=1))\n            scores = torch.abs(torch.norm(features, dim=1) - mean_norm) / (mean_norm + 1e-8)\n            # Lower score = rarer (outlier)\n            return scores\n            \n        elif self.update_strategy == 'statistical':\n            # Similar to rarity but uses running mean of norms (statistical tracking)\n            if self.num_updates > 0:\n                # Compute L2 norm for each sample\n                sample_norms = torch.norm(features, dim=1)\n                \n                # Use running mean of norms instead of batch mean\n                scores = torch.abs(sample_norms - self.running_mean_norm) / (self.running_mean_norm + 1e-8)\n                # Lower score = rarer (closer to running mean)\n                return scores\n            else:\n                return torch.zeros(batch_size, device=features.device)\n        \n        elif self.update_strategy == 'entropy':\n            # Based on prediction entropy (uncertainty)\n            # Requires passing logits/probabilities\n            # This is a placeholder - actual implementation needs logits\n            norm_features = F.normalize(features, dim=1)\n            entropy = -torch.sum(norm_features * torch.log(torch.abs(norm_features) + 1e-8), dim=1)\n            return -entropy  # Higher entropy = more uncertain = rarer\n        \n        elif self.update_strategy == 'diversity':\n            # Select features most different from current memory\n            if self.index > 0:\n                valid_memory = self.memory[:self.index]\n                norm_features = F.normalize(features, dim=1)\n                norm_memory = F.normalize(valid_memory, dim=1)\n                # Max similarity to any memory item\n                max_similarity = torch.matmul(norm_features, norm_memory.T).max(dim=1)[0]\n                # Lower similarity = more diverse\n                return max_similarity\n            else:\n                return torch.zeros(batch_size, device=features.device)\n        \n        elif self.update_strategy == 'hybrid':\n            # Combine rarity and diversity\n            # Rarity component\n            mean_norm = torch.mean(torch.norm(features, dim=1))\n            rarity = torch.abs(torch.norm(features, dim=1) - mean_norm) / (mean_norm + 1e-8)\n            \n            # Diversity component\n            if self.index > 0:\n                valid_memory = self.memory[:self.index]\n                norm_features = F.normalize(features, dim=1)\n                norm_memory = F.normalize(valid_memory, dim=1)\n                max_similarity = torch.matmul(norm_features, norm_memory.T).max(dim=1)[0]\n                diversity = 1 - max_similarity\n            else:\n                diversity = torch.ones(batch_size, device=features.device)\n            \n            # Weighted combination\n            scores = (1 - self.diversity_weight) * (-rarity) + self.diversity_weight * (-diversity)\n            return scores\n        \n        elif self.update_strategy == 'fifo':\n            # First In First Out - no scoring needed\n            return torch.zeros(batch_size, device=features.device)\n        \n        elif self.update_strategy == 'reservoir':\n            # Reservoir sampling - probabilistic\n            return torch.rand(batch_size, device=features.device)\n        \n        else:\n            raise ValueError(f\"Unknown update strategy: {self.update_strategy}\")\n\n    def update_statistics(self, features):\n        \"\"\"Update running mean and variance\"\"\"\n        batch_mean = features.mean(dim=0)\n        batch_var = features.var(dim=0)\n        \n        # For statistical strategy - update running mean of norms\n        if self.update_strategy == 'statistical':\n            sample_norms = torch.norm(features, dim=1)\n            batch_mean_norm = sample_norms.mean()\n            \n            if self.num_updates == 0:\n                self.running_mean_norm = batch_mean_norm\n            else:\n                self.running_mean_norm = self.momentum * self.running_mean_norm + (1 - self.momentum) * batch_mean_norm\n        \n        if self.num_updates == 0:\n            self.running_mean = batch_mean\n            self.running_var = batch_var\n        else:\n            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n        \n        self.num_updates += 1\n\n    def update(self, features, threshold=None):\n        \"\"\"Update memory bank with new features\"\"\"\n        batch_size = features.size(0)\n        \n        # Update running statistics\n        self.update_statistics(features)\n        \n        # Compute importance scores\n        scores = self.compute_importance_scores(features)\n        \n        # Apply threshold if specified\n        if threshold is None:\n            threshold = self.rarity_threshold\n        \n        if self.update_strategy in ['fifo', 'reservoir']:\n            # For FIFO and reservoir, store all or based on probability\n            if self.update_strategy == 'fifo':\n                mask = torch.ones(batch_size, dtype=torch.bool, device=features.device)\n            else:  # reservoir\n                # Reservoir sampling: probability decreases as memory fills\n                total_seen = self.num_updates * batch_size\n                probs = torch.minimum(\n                    torch.tensor(self.bank_size / (total_seen + 1e-8), device=features.device),\n                    torch.ones(batch_size, device=features.device)\n                )\n                mask = scores < probs  # scores are random in [0,1]\n        else:\n            # For other strategies, select based on threshold\n            if self.update_strategy in ['entropy', 'diversity', 'hybrid']:\n                # Lower scores are better (more rare/diverse)\n                mask = scores < torch.quantile(scores, threshold)\n            else:\n                # rarity, statistical\n                mask = scores < threshold\n        \n        selected_features = features[mask]\n        \n        if selected_features.size(0) > 0:\n            # If memory is full, replace oldest entries\n            if self.index + selected_features.size(0) > self.bank_size:\n                # Circular buffer - overwrite from beginning\n                remaining = self.bank_size - self.index\n                self.memory[self.index:] = selected_features[:remaining]\n                overflow = selected_features.size(0) - remaining\n                if overflow > 0:\n                    self.memory[:overflow] = selected_features[remaining:remaining + overflow]\n                    self.index = torch.tensor(overflow)\n                else:\n                    self.index = torch.tensor(self.bank_size)\n            else:\n                num_to_add = selected_features.size(0)\n                self.memory[self.index:self.index + num_to_add] = selected_features\n                self.memory_count[self.index:self.index + num_to_add] += 1\n                self.index = (self.index + num_to_add) % self.bank_size\n\n    def retrieve(self, query, k=3):\n        \"\"\"Retrieve relevant memories for query features\"\"\"\n        # Get valid memory entries\n        if self.index == 0:\n            return torch.zeros_like(query)\n        \n        valid_memory = self.memory[:self.index] if self.index < self.bank_size else self.memory\n        \n        norm_query = F.normalize(query, dim=1)\n        norm_memory = F.normalize(valid_memory, dim=1)\n        similarity = torch.matmul(norm_query, norm_memory.T)\n        \n        # Avoid self-similarity (similarity = 1)\n        mask = similarity < 0.9999\n        \n        k = min(k, valid_memory.size(0))\n        batch_size = query.size(0)\n        result = torch.zeros_like(query)\n        \n        for i in range(batch_size):\n            valid_indices = torch.where(mask[i])[0]\n            \n            if len(valid_indices) == 0:\n                continue\n            \n            valid_similarities = similarity[i, valid_indices]\n            k_valid = min(k, valid_similarities.size(0))\n            weights, rel_indices = valid_similarities.topk(k_valid)\n            abs_indices = valid_indices[rel_indices]\n            \n            retrieved = valid_memory[abs_indices]\n            weights = F.softmax(weights, dim=0).unsqueeze(1).expand_as(retrieved)\n            weighted_features = (retrieved * weights).sum(dim=0)\n            \n            result[i] = weighted_features\n            \n        return result\n\n\n# Main Model\nclass ChestXrayModel(nn.Module):\n    def __init__(self, num_classes, model_name='efficientnet_b0', dropout_rate=0.3, \n                 bank_size=512, update_strategy='hybrid', rarity_threshold=0.2,\n                 diversity_weight=0.5, memory_momentum=0.9):\n        super(ChestXrayModel, self).__init__()\n\n        # Backbone and Final Block\n        if model_name == 'resnet50':\n            self.base_model = models.resnet50(pretrained=True)\n            self.backbone = nn.Sequential(\n                self.base_model.conv1, self.base_model.bn1, self.base_model.relu,\n                self.base_model.maxpool, self.base_model.layer1, self.base_model.layer2,\n                self.base_model.layer3\n            )\n            self.final_block = self.base_model.layer4\n            self.feature_dim = 2048\n        elif model_name == 'densenet121':\n            self.base_model = models.densenet121(pretrained=True)\n            features = list(self.base_model.features.children())\n            self.backbone = nn.Sequential(*features[:-1])\n            self.final_block = nn.Sequential(features[-1])\n            self.feature_dim = 1024\n        elif model_name in ['efficientnet_b0', 'efficientnet_b1']:\n            self.base_model = models.efficientnet_v2_s(pretrained=True) if model_name == 'efficientnet_b0' else models.efficientnet_b1(pretrained=True)\n            features = list(self.base_model.features)\n            self.backbone = nn.Sequential(*features[:-1])\n            self.final_block = nn.Sequential(features[-1])\n            self.feature_dim = self.base_model.features[-1][0].out_channels\n        else:\n            raise ValueError(f\"Model {model_name} not supported\")\n\n        self.base_model.fc = nn.Identity() if hasattr(self.base_model, 'fc') else None\n        self.base_model.classifier = nn.Identity() if hasattr(self.base_model, 'classifier') else None\n\n        # Memory Bank with configurable strategy\n        self.memory_bank = MemoryBank(\n            self.feature_dim, \n            bank_size=bank_size,\n            update_strategy=update_strategy,\n            rarity_threshold=rarity_threshold,\n            diversity_weight=diversity_weight,\n            momentum=memory_momentum\n        )\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.BatchNorm1d(self.feature_dim),\n            nn.Linear(self.feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, num_classes)\n        )\n        self.model_name = model_name\n\n    def forward(self, x):\n        # Extract features\n        backbone_features = self.backbone(x)\n        main_features = self.final_block(backbone_features)\n\n        roi_features = main_features\n        roi_pooled = F.adaptive_avg_pool2d(roi_features, (1, 1)).flatten(1)\n\n        # Fused features\n        fused_features = roi_pooled\n\n        # Update and retrieve from memory bank\n        if self.training:\n            self.memory_bank.update(fused_features.detach())\n        \n        memory_features = self.memory_bank.retrieve(fused_features)\n        enhanced_features = fused_features + memory_features\n\n        # Classification\n        out = self.classifier(enhanced_features)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.874322Z","iopub.execute_input":"2025-11-04T16:33:57.874620Z","iopub.status.idle":"2025-11-04T16:33:57.905846Z","shell.execute_reply.started":"2025-11-04T16:33:57.874596Z","shell.execute_reply":"2025-11-04T16:33:57.905144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Script","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nimport torch.nn as nn\nimport torch\nimport torchvision.models as models\nfrom copy import deepcopy\n\n# Focal Loss Implementation\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        \"\"\"\n        Focal Loss to address class imbalance and hard sample mining\n        \n        Args:\n            alpha (float): Weighting factor for positive samples\n            gamma (float): Focusing parameter\n            reduction (str): Reduction method ('mean', 'sum', or 'none')\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Compute focal loss\n        \n        Args:\n            inputs (torch.Tensor): Model predictions (logits)\n            targets (torch.Tensor): Ground truth labels\n        \n        Returns:\n            torch.Tensor: Computed loss\n        \"\"\"\n        # Apply sigmoid to convert logits to probabilities\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        \n        # Focal Loss modification\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n        \n        if self.reduction == 'mean':\n            return torch.mean(F_loss)\n        elif self.reduction == 'sum':\n            return torch.sum(F_loss)\n        else:\n            return F_loss\n\n# Asymmetric Loss Implementation\nclass AsymmetricLoss(nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, reduction='mean'):\n        \"\"\"\n        Asymmetric Loss to handle class imbalance and hard negative mining\n        \n        Args:\n            gamma_neg (float): Focusing parameter for negative samples\n            gamma_pos (float): Focusing parameter for positive samples\n            clip (float): Clip the predictions to prevent extreme values\n            eps (float): Small epsilon to prevent log(0)\n            reduction (str): Reduction method ('mean', 'sum', or 'none')\n        \"\"\"\n        super(AsymmetricLoss, self).__init__()\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.eps = eps\n        self.reduction = reduction\n\n    def forward(self, x, y):\n        \"\"\"\n        Compute asymmetric loss\n        \n        Args:\n            x (torch.Tensor): Model predictions (logits)\n            y (torch.Tensor): Ground truth labels\n        \n        Returns:\n            torch.Tensor: Computed loss\n        \"\"\"\n        # Convert to probabilities\n        x_sigmoid = torch.sigmoid(x)\n        \n        # Clip predictions to prevent extreme values\n        xs_min = x_sigmoid.clamp(min=self.eps)\n        xs_max = x_sigmoid.clamp(max=1-self.eps)\n        \n        # Asymmetric term for positive and negative samples\n        loss_pos = -y * torch.log(xs_min) * torch.pow(1 - xs_min, self.gamma_pos)\n        loss_neg = -(1 - y) * torch.log(1 - xs_max) * torch.pow(xs_max, self.gamma_neg)\n        \n        loss = loss_pos + loss_neg\n        \n        if self.reduction == 'mean':\n            return torch.mean(loss)\n        elif self.reduction == 'sum':\n            return torch.sum(loss)\n        else:\n            return loss\n\n# Create a custom fastai Learner for ChestXrayModel\ndef create_fastai_learner(\n    dls,                            # DataLoaders object\n    num_classes=14,                 # Number of output classes\n    lr=1e-4,                        # Learning rate\n    momentum=0.9,                   # Momentum for the momentum encoder (aligned with model)\n    dropout_rate=0.3,               # Dropout rate for classifier\n    mixup=False,                    # Whether to use mixup augmentation\n    wd=1e-2,                        # Weight decay\n    model=None,                     # Pass a pre-instantiated model if you have one\n    cbs=None,                       # Additional callbacks\n    warmup_epochs=0,                # Number of warm-up epochs for momentum encoder\n    loss_type='focal',              # Loss type: 'focal' or 'asymmetric'\n    focal_alpha=1,                  # Focal loss alpha parameter\n    focal_gamma=2,                  # Focal loss gamma parameter\n    asymmetric_gamma_neg=4,         # Asymmetric loss gamma for negative samples\n    asymmetric_gamma_pos=1          # Asymmetric loss gamma for positive samples\n):\n    # Create model if not provided\n    if model is None:\n        #model = ChestXrayModel(\n        #    num_classes=num_classes,\n        #    dropout_rate=dropout_rate\n            \n        #)\n        #Statistical outlier detection\n        model = ChestXrayModel(\n            num_classes=14,\n            update_strategy='rarity',\n        )\n        \n        # Pure diversity\n        #model = ChestXrayModel(\n        #    num_classes=14,\n        #    update_strategy='diversity'\n        #)\n    \n    # Register a custom callback to update momentum encoder with warm-up\n    class MomentumUpdateCallback(Callback):\n        def __init__(self, warmup_epochs):\n            super().__init__()\n            self.warmup_epochs = warmup_epochs\n        \n        def after_batch(self):\n            if hasattr(self.learn.model, 'momentum_final_block'):\n                # Apply warm-up during the first few epochs\n                is_warmup = self.learn.epoch < self.warmup_epochs\n                self.learn.model.momentum_final_block.update(\n                    self.learn.model.final_block, warmup=is_warmup\n                )\n    \n    # Define a custom loss function with multiple loss options\n    class ChestXrayLoss(Module):\n        def __init__(self, loss_type, **kwargs):\n            super().__init__()\n            if loss_type == 'focal':\n                self.loss = FocalLoss(\n                    #alpha=kwargs.get('focal_alpha', 1),\n                    #gamma=kwargs.get('focal_gamma', 2)\n                )\n            elif loss_type == 'asymmetric':\n                self.loss = AsymmetricLoss(\n                    gamma_neg=kwargs.get('asymmetric_gamma_neg', 4),\n                    gamma_pos=kwargs.get('asymmetric_gamma_pos', 1)\n                )\n            elif loss_type == 'bce':\n                self.loss = nn.BCEWithLogitsLoss()\n            else:\n                raise ValueError(f\"Unsupported loss type: {loss_type}\")\n        \n        def forward(self, preds, targets):\n            return self.loss(preds, targets)\n    \n    # Prepare default callbacks\n    default_cbs = [\n        MomentumUpdateCallback(warmup_epochs),  # Custom callback with warm-up\n        SaveModelCallback(monitor='valid_loss'),  # Save best model\n        EarlyStoppingCallback(monitor='valid_loss', patience=3)  # Early stopping\n    ]\n    \n    # Add user-specified callbacks\n    if cbs is not None:\n        if isinstance(cbs, list):\n            default_cbs.extend(cbs)\n        else:\n            default_cbs.append(cbs)\n    \n    # Create the learner with custom model and loss\n    learn = Learner(\n        dls, \n        model, \n        loss_func=ChestXrayLoss(\n            loss_type=loss_type, \n            focal_alpha=focal_alpha, \n            focal_gamma=focal_gamma,\n            asymmetric_gamma_neg=asymmetric_gamma_neg,\n            asymmetric_gamma_pos=asymmetric_gamma_pos\n        ),\n        metrics=[accuracy_multi, F1ScoreMulti(), RocAucMulti()],  # Multi-label metrics\n        wd=wd,\n        cbs=default_cbs\n    )\n    \n    # Modify the model's forward method to work with fastai's expectations\n    class ModelWrapper(nn.Module):\n        def __init__(self, model):\n            super().__init__()\n            self.model = model\n        \n        def forward(self, x):\n            return self.model(x)  # Model returns only class predictions\n    \n    # Wrap the model\n    learn.model = ModelWrapper(learn.model)\n    \n    # Enable mixed precision training if available\n    learn.to_fp16()\n    \n    # Add mixup if requested (suitable for multi-label with BCE)\n    if mixup:\n        learn.add_cb(MixUp())\n    \n    # Add a progress bar callback for better training visualization\n    learn.add_cb(ProgressCallback())\n    \n    # Add CSV logger to track metrics\n    learn.add_cb(CSVLogger())\n    \n    return learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.908924Z","iopub.execute_input":"2025-11-04T16:33:57.909171Z","iopub.status.idle":"2025-11-04T16:33:57.929972Z","shell.execute_reply.started":"2025-11-04T16:33:57.909153Z","shell.execute_reply":"2025-11-04T16:33:57.929154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\n\ncbs=[\n    SaveModelCallback(monitor='valid_loss', min_delta=0.0001, with_opt=True),\n    EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001, patience=5),\n    ShowGraphCallback()\n    ]\n\nlearn = create_fastai_learner(dls,cbs=cbs,loss_type='bce')\n\n#learn.model = torch.nn.DataParallel(learn.model)","metadata":{"_uuid":"34c26c3d-07b7-4968-8b7d-df7e4df04087","_cell_guid":"f0d356db-5bae-42d6-8058-836d0741f640","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:57.930913Z","iopub.execute_input":"2025-11-04T16:33:57.931191Z","iopub.status.idle":"2025-11-04T16:33:58.434373Z","shell.execute_reply.started":"2025-11-04T16:33:57.931167Z","shell.execute_reply":"2025-11-04T16:33:58.433726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\nprint('intial learning rate=', lrs.minimum)","metadata":{"_uuid":"30a79388-5c62-494d-bfe2-3ecbb65c5c39","_cell_guid":"26650f8e-2d78-4de6-b739-3f26ed1b84c9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:33:58.435786Z","iopub.execute_input":"2025-11-04T16:33:58.435993Z","iopub.status.idle":"2025-11-04T16:35:37.259874Z","shell.execute_reply.started":"2025-11-04T16:33:58.435977Z","shell.execute_reply":"2025-11-04T16:35:37.258844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('intial learning rate=', lrs.valley)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:35:37.260960Z","iopub.execute_input":"2025-11-04T16:35:37.261244Z","iopub.status.idle":"2025-11-04T16:35:37.266361Z","shell.execute_reply.started":"2025-11-04T16:35:37.261216Z","shell.execute_reply":"2025-11-04T16:35:37.265539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.unfreeze()\nlearn.model\nsum(p.numel() for p in learn.model.parameters() if p.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:35:37.267124Z","iopub.execute_input":"2025-11-04T16:35:37.267359Z","iopub.status.idle":"2025-11-04T16:35:37.290414Z","shell.execute_reply.started":"2025-11-04T16:35:37.267339Z","shell.execute_reply":"2025-11-04T16:35:37.289670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.fine_tune(freeze_epochs=3,epochs=20, base_lr=lrs.valley)","metadata":{"_uuid":"a73be5d6-6cb3-45c0-915c-265c743c8c86","_cell_guid":"acbf3c2b-5373-4e90-9dc8-ccd94c68ee70","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:35:37.291166Z","iopub.execute_input":"2025-11-04T16:35:37.291507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.save('fastai_momentum_inner_gate_70_20_10_effnetb1')","metadata":{"_uuid":"68eeb659-73b8-47d5-8383-c6b366514a2f","_cell_guid":"6cf3994e-b519-4c62-aa13-d8800163f2c5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#learn = learn.load('/kaggle/working/models/fastai_momentum_cross_spatial_70_20_10')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 2","metadata":{}},{"cell_type":"code","source":"import fastai\nfrom fastai.vision.all import *\nfrom tqdm import tqdm\nfrom glob import glob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#learn = learn.load('/kaggle/working/models/fastai_momentum_cross_spatial_70_20_10')\nSEED = 85\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)\nlabels_train_val = pd.read_csv('/kaggle/input/data/train_val_list.txt')\nlabels_train_val.columns = ['Image_Index']\nlabels_test = pd.read_csv('/kaggle/input/data/test_list.txt')\nlabels_test.columns = ['Image_Index']\ndisease_labels = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n'Cardiomegaly', 'Nodule', 'Mass', 'Hernia']\n# NIH Dataset Labels CSV File \nlabels_df = pd.read_csv('/kaggle/input/data/Data_Entry_2017.csv')\nlabels_df.columns = ['Image_Index', 'Finding_Labels', 'Follow_Up_#', 'Patient_ID',\n                  'Patient_Age', 'Patient_Gender', 'View_Position',\n                  'Original_Image_Width', 'Original_Image_Height',\n                  'Original_Image_Pixel_Spacing_X',\n                  'Original_Image_Pixel_Spacing_Y', 'dfd']\n# One hot encoding\nfor diseases in tqdm(disease_labels): \n    labels_df[diseases] = labels_df['Finding_Labels'].map(lambda result: 1 if diseases in result else 0)\n\n# labels_df.to_csv('/kaggle/working/newData.csv')\n# labels_df=labels_df[labels_df.Finding_Labels != 'No Finding']\n# #labels_df.head(3)\n\nlabels_df['Finding_Labels'] = labels_df['Finding_Labels'].apply(lambda s: [l for l in str(s).split('|')])\n\nnum_glob = glob('/kaggle/input/data/*/images/*.png')\nimg_path = {os.path.basename(x): x for x in num_glob}\n\nlabels_df['Paths'] = labels_df['Image_Index'].map(img_path.get)\nlabels_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_patients = np.unique(labels_df['Patient_ID'])\nlen(unique_patients)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# train-70\n# val-10\n# test-20\ntrain_val_df_patients, test_df_patients = train_test_split(unique_patients, \n                                   test_size = 0.2,\n                                   random_state = SEED,\n                                    shuffle= True\n                                   )\nlen(train_val_df_patients)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_val_df = labels_df[labels_df['Patient_ID'].isin(train_val_df_patients)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_df.shape\nprint('train_val size', train_val_df.shape[0])\nprint('test size', labels_df.shape[0] - train_val_df.shape[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"item_transforms = [\n    Resize((224, 224)),\n]\n\nbatch_transforms = [\n    Flip(),\n    Rotate(),\n    Normalize.from_stats(*imagenet_stats),\n]\n\n\ndef get_x(row):\n    return row['Paths']\n\ndef get_y(row):\n    labels = row[disease_labels].tolist()\n    return labels\n\ndblock = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock(encoded=True,vocab=disease_labels)),\n                   splitter=RandomSplitter(valid_pct=0.125, seed=SEED),\n                   get_x=get_x,\n                   get_y=get_y,\n                   item_tfms=item_transforms,\n                   batch_tfms=batch_transforms\n                  )\ndls = dblock.dataloaders(train_val_df, bs=128)\n# print(dblock.datasets(train_val_merge).train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastai.vision.all import *\n\ncbs=[\n    SaveModelCallback(monitor='valid_loss', min_delta=0.0001, with_opt=True),\n    EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001, patience=5),\n    ShowGraphCallback()\n    ]\n\nlearn = create_fastai_learner(dls,cbs=cbs,momentum=0.9999,loss_type='bce')\nlearn = learn.load('/kaggle/working/models/model')\n#learn.model = torch.nn.DataParallel(learn.model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(5, slice(2e-5, 8e-5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.save('fastai_momentum_cross_spatial_70_20_10_stage2')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ndef get_roc_auc(learner):\n    #arch = model_arch\n    # learner = vision_learner(dls, arch, metrics=[accuracy_multi, F1ScoreMulti(), RocAucMulti()])\n    # learner.model = torch.nn.DataParallel(learner.model)\n    # learner.load(model_path)\n    # learner.to('cuda')\n    learner.freeze()\n    preds, y_test = learner.get_preds(ds_idx=1)\n    roc_auc = roc_auc_score(y_test, preds)\n    \n    scores=[]\n    for i in range(0,14):\n        label_roc_auc_score=roc_auc_score(y_test[:,i],preds[:,i])\n        scores.append(label_roc_auc_score)\n    print('ROC_AUC_Labels:', list(zip(disease_labels,scores)))   \n    \n#     print('AVERAGE', sum(scores)/len(scores))\n    print(f'SCORE: {roc_auc}')\n    del learner\n    #gc.collect()\n    return {\n        'roc_auc': roc_auc,\n        'preds': preds,\n        'y_test': y_test\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"modelv1_result= get_roc_auc(learn)\npreds = modelv1_result['preds']\ntorch.save(preds, 'modelv1_result.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score\n\ndef get_roc_auc(learner, threshold=0.5):\n    learner.freeze()\n    preds, y_test = learner.get_preds(ds_idx=1)\n    \n    # Calculate ROC AUC (this doesn't actually use a threshold, it considers all possible thresholds)\n    roc_auc = roc_auc_score(y_test, preds)\n    \n    # Apply threshold to get binary predictions\n    binary_preds = (preds > threshold).float()\n    \n    # Calculate other metrics with the chosen threshold\n    precision = precision_score(y_test.cpu().numpy(), binary_preds.cpu().numpy(), average='macro', zero_division=0)\n    recall = recall_score(y_test.cpu().numpy(), binary_preds.cpu().numpy(), average='macro', zero_division=0)\n    f1 = f1_score(y_test.cpu().numpy(), binary_preds.cpu().numpy(), average='macro', zero_division=0)\n    \n    # Per-class metrics with custom threshold\n    scores = []\n    precision_scores = []\n    recall_scores = []\n    f1_scores = []\n    \n    for i in range(0, 14):\n        # ROC AUC per class (threshold-independent)\n        label_roc_auc_score = roc_auc_score(y_test[:, i], preds[:, i])\n        scores.append(label_roc_auc_score)\n        \n        # Precision, recall, F1 with custom threshold\n        class_precision = precision_score(y_test[:, i].cpu().numpy(), binary_preds[:, i].cpu().numpy(), zero_division=0)\n        class_recall = recall_score(y_test[:, i].cpu().numpy(), binary_preds[:, i].cpu().numpy(), zero_division=0)\n        class_f1 = f1_score(y_test[:, i].cpu().numpy(), binary_preds[:, i].cpu().numpy(), zero_division=0)\n        \n        precision_scores.append(class_precision)\n        recall_scores.append(class_recall)\n        f1_scores.append(class_f1)\n    \n    print(f'Using threshold: {threshold}')\n    print('ROC_AUC_Labels:', list(zip(disease_labels, scores)))   \n    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n    print(f'ROC AUC Score: {roc_auc:.4f}')\n    \n    del learner\n    #gc.collect()\n    return {\n        'roc_auc': roc_auc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'class_scores': {\n            'roc_auc': scores,\n            'precision': precision_scores,\n            'recall': recall_scores,\n            'f1': f1_scores\n        },\n        'preds': preds,\n        'y_test': y_test,\n        'binary_preds': binary_preds\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}